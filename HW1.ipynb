{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Information Retrieval\n",
    "## Instructions\n",
    "1. Students will form teams of three people each and submit a single homework for each team in the format - ID1_ID2_ID3.ipynb\n",
    "2. Groups of three!\n",
    "2. **Do not write your names anywhere.**\n",
    "3. For the code part: \n",
    "> **Write your code only in the mentioned sections. Do not change the code of other sections**. Do not use any imports unless we say so.\n",
    "4. For theoretical questions, if any - write your answer in the markdown cell dedicated to this task, in **English**.\n",
    "\n",
    "\n",
    "#### Deviation from the aforementioned  instructions will lead to reduced grade\n",
    "---\n",
    "\n",
    "\n",
    "## Clarifications\n",
    "1. The same score for the homework will be given to each member of the team.  \n",
    "2. The goal of this homework is to test your understanding of the concepts presented in the lectures. \\\n",
    "Anyhow, we provide here detailed explanations for the code part and if you have problems - ask.\n",
    "3. Questions can be sent to the forum, you are encouraged to ask questions but do so after you have been thinking about your question. \n",
    "4. The length of the empty gaps (where you are supposed to write your code) is a recommendation (the amount of space took us to write the solution) and writing longer code will not harm your grade. We do not expect you to use the programming tricks and hacks we used to make the code shorter.   \n",
    "Having said that, we do encourage you to write good code and keep that in mind - **extreme** cases may be downgraded.  \n",
    "We also encourage to use informative variable names - it is easier for us to check and for you to understand. \n",
    "\n",
    "For your convenience, , the code has a **DEBUG** mode that you may use in order to debug with toy data.  \n",
    "It is recommended to solve the code in that mode (with efficiency in mind) and then run the code on all the data.\n",
    "**Do not forget to file the HW with DEBUG == False**.\n",
    "\n",
    "\n",
    "Since it is the first time we provide this homework please notify us if there is a bug/something is unclear, typo's exc..\n",
    "\n",
    "5. We use Python 3.7 for programming.\n",
    "6. Make sure you have all the packages and functions used in the import section. Most of it is native to Anaconda Python distribution.\n",
    "\n",
    "### Have fun !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from typing import List,Dict\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from pathlib import Path\n",
    "import statistics\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter() #Start timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omery\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omery\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from string import punctuation, ascii_lowercase\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n",
    "\"\"\" you can change this cell \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\"\"\"\n",
    "Recommended to start with a small number to get a feeling for the preprocessing with prints (N_ROWS_FOR_DEBUG = 2)\n",
    "later increase this number for 5*10**3 in order to see that the code runs at reasonable speed. \n",
    "When setting Debug == False, our code implements bow.fit() in 15-20 minutes according to the tqdm progress bar. Your solution is not supposed to be much further than that.\n",
    "\"\"\"\n",
    "N_ROWS_FOR_DEBUG = 5*10**3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = Path(\"lyrics.csv\")\n",
    "BOW_PATH = Path(\"bow.csv\")\n",
    "N_ROWS = N_ROWS_FOR_DEBUG if DEBUG else None\n",
    "CHUNCK_SIZE = 5 if DEBUG else 5*10**3\n",
    "tqdm_n_iterations = N_ROWS//CHUNCK_SIZE +1 if DEBUG else 114*10**3//CHUNCK_SIZE + 1\n",
    "COLS = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of words /TfIdf model\n",
    "### Implement the following methods:\n",
    "\n",
    "* `preprocess_sentence`: \n",
    "    * Lower case the word\n",
    "    * Ignores it if it's in the stopwords list\n",
    "    * Removes characters which are not in the allowed symbols\n",
    "    * Stems it and appends it to the output sentence\n",
    "    * Discards words with length <= 1\n",
    "    \n",
    "    \n",
    "* `update_counts_and_probabilities`: \n",
    "\n",
    "    * Update self.unigram count (the amount of time each word is in the text)\n",
    "    * Update self.bigram count (two consecutive word occurances)\n",
    "    * Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}   \n",
    "    \n",
    "* `compute_word_document_frequency`:\n",
    "\n",
    "   * For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "$$\\sum_{i \\in docs} I(apple \\in doc_i), I := Indicator function$$\n",
    "\n",
    "\n",
    "* `update_inverted_index_with_tf_idf_and_compute_document_norm`:\n",
    "\n",
    "    * Update the inverted index (which currently hold word counts) with tf idf weighing. We will compute tf by dividing with the number of words in each document. \n",
    "    * As we want to calculate the document norm, incrementally update the document norm. pay attention that later we apply sqrt to it to finish the process.\n",
    "\n",
    "#### The result of this code is a bag of words model that already counts for TF-IDF weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allowed_symbols = set(l for l in ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [15:32<00:00, 40.55s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):                           \n",
    "        lower_word=word.lower()            # Lower case the word\n",
    "        if lower_word not in stop_words:   # Ignores it if it's in the stopwords list\n",
    "            word_letters = list(lower_word)\n",
    "            valid_letters = []\n",
    "            for l in word_letters:\n",
    "                if l in allowed_symbols:\n",
    "                    valid_letters.append(l)  # Removes characters which are not in the allowed symbols\n",
    "            clean_word = ''.join(valid_letters)  # Reconnect the good letters back into one word\n",
    "            clean_word = stemmer.stem(clean_word) # stem of the word\n",
    "            if len(clean_word) > 1:             # Remove words that are 1 character long\n",
    "                output_sentence.append(clean_word)\n",
    "    return output_sentence    \n",
    "\n",
    "def get_data_chuncks() -> List[str]:\n",
    "    for i ,chunck in enumerate(pd.read_csv(INPUT_FILE_PATH, usecols = COLS, chunksize = CHUNCK_SIZE, nrows = N_ROWS)):\n",
    "        chunck = chunck.values.tolist()\n",
    "        yield [chunck[i][0] for i in range(len(chunck))]\n",
    "        \n",
    "class TfIdf:\n",
    "    def __init__(self,  weighted_dic = None):\n",
    "        self.weighted_dic = weighted_dic\n",
    "        self.unigram_count =  Counter() # כל מילה כמה מופיעה בקורפוס\n",
    "        self.bigram_count = Counter() # כל צמד מילים כמה פעמים מופיעות בקורפוס\n",
    "        self.document_term_frequency = Counter() # כמות המילים שיש בכל שיר\n",
    "        self.word_document_frequency = {} # כל מילה בקורפוס בכמה שירים היא מופיעה\n",
    "        self.inverted_index = {}\n",
    "        self.doc_norms = {}\n",
    "        self.n_docs = -1\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        self.bow_path = BOW_PATH\n",
    "\n",
    "    def update_counts_and_probabilities(self, sentence, document_id):\n",
    "        \"\"\"\n",
    "        Update the number of times a word appears in every document\n",
    "        \"\"\"\n",
    "        sentence_len = len(sentence)\n",
    "        self.document_term_frequency[document_id] = sentence_len\n",
    "        for i,word in enumerate(sentence):\n",
    "            self.unigram_count[word] += 1                                           #update the unigram Counter object that holds for each word how many time it appers in the corpus\n",
    "            if i < (sentence_len - 1):                                              #edge condition to check that we are not in the last word\n",
    "                bigram = word,sentence[i+1]                                         #take the word and the adjacent word in the sentence\n",
    "                self.bigram_count[bigram] += 1                                        #update the bigram Counter object that holds for each pair of words how many time it appers in the corpus\n",
    "            if word not in self.inverted_index:\n",
    "                self.inverted_index[word] = {}\n",
    "            if document_id not in self.inverted_index[word]:\n",
    "                self.inverted_index[word][document_id] = 1\n",
    "            else:\n",
    "                self.inverted_index[word][document_id] += 1\n",
    "            \n",
    "    def fit(self):\n",
    "        for chunck in tqdm(get_data_chuncks(), total = tqdm_n_iterations):\n",
    "            for sentence in chunck: # sentence is a song (string)\n",
    "                self.n_docs += 1 \n",
    "                if not isinstance(sentence, str):\n",
    "                    continue\n",
    "                sentence = self.sentence_preprocesser(sentence)\n",
    "                if sentence:\n",
    "                    self.update_counts_and_probabilities(sentence,self.n_docs)\n",
    "        self.save_bow() # bow is 'bag of words'\n",
    "        self.compute_word_document_frequency()\n",
    "        self.update_inverted_index_with_tf_idf_and_compute_document_norm()\n",
    "             \n",
    "    def compute_word_document_frequency(self):\n",
    "        \"\"\"\n",
    "        Count the number of documents a specific words appears\n",
    "        \"\"\"\n",
    "        for word in self.inverted_index.keys():\n",
    "            self.word_document_frequency[word] = len(self.inverted_index[word])\n",
    "            \n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        for word in self.inverted_index.keys(): # For every word in the corpus\n",
    "            for doc in self.inverted_index[word].keys():  # For every document containing the word\n",
    "                n_words_in_doc = self.document_term_frequency[doc] # Length of the document is the number of words in it\n",
    "                tf = (self.inverted_index[word][doc]) / n_words_in_doc  # Term frequency is the number of words in a docuemt divided by the number of the words in it\n",
    "                idf = np.log10((self.n_docs + 1) / self.word_document_frequency[word])\n",
    "                w_ij = tf * idf # Calcluated tf-idf metric              \n",
    "                self.inverted_index[word][doc] = w_ij # Set the weight of a document to the tf-idf metric value   \n",
    "                if doc not in self.doc_norms:\n",
    "                    self.doc_norms[doc] = 0  # initialize value for the first time that creating the doc norm\n",
    "                self.doc_norms[doc] += (w_ij ** 2) # To calculate the norm, sum the square of the weights \n",
    "        for doc in self.doc_norms.keys():\n",
    "            self.doc_norms[doc] = np.sqrt(self.doc_norms[doc]) # The norm of the document is the square root of the weights' sum of squares\n",
    "            \n",
    "    def save_bow(self):\n",
    "        pd.DataFrame([self.inverted_index]).T.to_csv(self.bow_path)\n",
    "                \n",
    "tf_idf = TfIdf()\n",
    "tf_idf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Bag of words model:\n",
    "\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "D - The number of docs in the corpus\n",
    "V - The number of all the words in the corpus =200 (We assumed that 200 is the average number of words in one song)\n",
    "# Assumption1 - word.lower complexity is O(1), as words are not very long O(8)~=O(1)\n",
    "1. \n",
    "\n",
    "# Fit complexity\n",
    "    # Preprocess_Sentece - O(V) # tokenize evey sentence - insert every word to a list\n",
    "        # Look for word in set - O(1)\n",
    "        # Convert word to list O(1) (Assumption1)\n",
    "        # Iterate over letters in word * look if letter is allowed- O(1) * O(1)\n",
    "        # Stemming - O(1)\n",
    "        # Total complexity - O(V) * O(1) * O(1) * O(1) = O(V)\n",
    "        # Number of preprocess works on every word O(V) (Total number of words in the document)\n",
    "        \n",
    "    # update_counts_and_probabilities complexity:\n",
    "    # For every word in the corpus O(V)\n",
    "        # Calculate unigram O(1)\n",
    "        # Calculate bigram O(1)\n",
    "        # Calculate trigram O(1)\n",
    "        # Store in dictionary O(1)\n",
    "    # Total complexity = O(V) * ((O(1) * 4 ) = O(V)\n",
    "\n",
    "    # Compute complexity\n",
    "        # Iterate over every word in the corpus O(V)\n",
    "# For every sentence (document) call preprocess and call update_counts_and_probabilities, then call compute_word_document_frequency:\n",
    "# (O(D) * (O(V) + O(V))) + O(V) => O(D) * O(V) + O(V) => O(D * V) + O(V) = O(D * V)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DocumentRetriever\n",
    "Not this retriever &#8595;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![dsafdsafsdafdsf](https://cdn3-www.dogtime.com/assets/uploads/2019/10/golden-cocker-retriever-mixed-dog-breed-pictures-cover-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the following methods:\n",
    "\n",
    "`reduce_query_to_counts`: given a list of words returns a counter object with words as keys and counts as values.\n",
    "\n",
    "`rank`: given a query and relevant documents calculate the similarity (cosine or inner product simialrity) between each document and the query.   \n",
    "Make sure to transform the query word counts to tf idf as well. \n",
    "\n",
    "`sort_and_retrieve_k_best`: returns the top k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, tf_idf):\n",
    "        self.sentence_preprocesser = preprocess_sentence  \n",
    "        self.vocab = set(tf_idf.unigram_count.keys())\n",
    "        self.n_docs = tf_idf.n_docs\n",
    "        self.inverted_index = tf_idf.inverted_index\n",
    "        self.word_document_frequency = tf_idf.word_document_frequency\n",
    "        self.doc_norms = tf_idf.doc_norms\n",
    "        \n",
    "    def rank(self,query: Dict[str,int], documents: Dict[str,Counter], metric: str ) -> Dict[str, float]:\n",
    "        result = {} # key: DocID , value : float , simmilarity to query\n",
    "        query_len = np.sum(np.array(list(query.values())))\n",
    "        tfidf_sum_sqrt = 0 # initialize the query norm\n",
    "        query_tfidf = {} # initialize dict that holds the query words and their tf-idf weighting \n",
    "        for word in query:\n",
    "            tf = query[word] / query_len              #term frequency of the word is the number of times that the word appers in the query divided in the number of words in the query\n",
    "            idf = np.log10((self.n_docs+1)/self.word_document_frequency[word]) #calcluated tf idf weighting: term frequency double in idf(log 10 of the number of docs, divded in the number of docs that the word appers in )\n",
    "            query_tfidf[word] = tf * idf\n",
    "            tfidf_sum_sqrt += (query_tfidf[word]) ** 2        #in order to calculated the norm, we will sum up the square weights and in the end take the sqrt \n",
    "            for doc in documents[word]:    #for each doc from the docs that the word appears in\n",
    "                if doc not in result:             #first time the doc appears in the final dict result\n",
    "                    result[doc] = 0                 #initialize the doc value to zero\n",
    "                result[doc] += (documents[word][doc] * query_tfidf[word]) #sum up the inner_product calculation to the result dict (word-dict weighting duble the word-query weighting)      \n",
    "        query_norm = np.sqrt(tfidf_sum_sqrt)\n",
    "        if metric == 'cosine':\n",
    "            for doc in result:\n",
    "                result[doc] = result[doc] / (query_norm * self.doc_norms[doc])   #doc_norms is already sqrt fron Q.1.1\n",
    "        return result\n",
    "        \n",
    "    def sort_and_retrieve_k_best(self, scores: Dict[str, float],k :int):\n",
    "        return list(dict(Counter(scores).most_common(k)).keys())\n",
    "  \n",
    "    def reduce_query_to_counts(self, query : List)->  Counter:\n",
    "        return Counter(query)\n",
    "\n",
    "    def get_top_k_documents(self,query : str, metric: str , k = 5) -> List[str]:\n",
    "        query = self.sentence_preprocesser(query)\n",
    "        query = [word for word in query if word in self.vocab] # filter nan \n",
    "        query_bow = self.reduce_query_to_counts(query)\n",
    "        relavant_documents = {word : self.inverted_index.get(word) for word in query}\n",
    "        ducuments_with_similarity = self.rank(query_bow,relavant_documents, metric)\n",
    "        return self.sort_and_retrieve_k_best(ducuments_with_similarity,k)\n",
    "        \n",
    "dr = DocumentRetriever(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omery\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\"\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50221, 50233, 28634, 101537, 38867]\n",
      "[111535, 111650, 26464, 38867, 7344]\n"
     ]
    }
   ],
   "source": [
    "cosine_top_k = dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "song #0 \n",
      "A Town Called Malice. Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #1 \n",
      "A Town Called Malice. Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #2 \n",
      "Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #3 \n",
      "Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #4 \n",
      "Rosie whatcha doing in this low class joint,. dancing in the dark all day. You used to be the darling of your high school scene. Now you put your love on display. Sweaty hands hand you up a dollar bill.. Hungry eyes never seem to get their fill. I used to watch you walking down the hall.. Rosie do you see me when you hear them call your name.. Rosie, Rosie I wanna take you away.. Rosie, Rosie I'm gonna make you mine someday.. Rosie I went with you for that rose tattoo. You promised no one else would see. I used to wait and drive you home from dancing school.. Remember when you danced just for me. Our love was deeper than the night was long. But things just didn't work out like our favorite song. I used to watch you walking down the hall. Rosie do you see me when you hear them call your name. Rosie, Rosie, I wanna take you away.. Rosie, Rosie, I'm gonna make you mine someday. Rosie, Rosie. Do you remember our love was deeper than the night was long. But things just didn't work out like our favorite song. I used to watch you walking down the hall. Rosie do you see me when you hear them call your name--ROSIE. Rosie, Rosie, I wanna take you away.. Rosie, Rosie, I'm gonna make you mine someday. Rosie, Rosie. \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for index, song in enumerate(pd.read_csv(INPUT_FILE_PATH,usecols = [0]).iloc[cosine_top_k]['Lyric']):\n",
    "    sep = \"#\"*50\n",
    "    print(F\"{sep}\\nsong #{index} \\n{song} \\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 term statistics:\n",
    "Use \"tf_idf\" object that we created earlier and answer the following questions:\n",
    "\n",
    "1. How many unique words we have?\n",
    "2. How many potential word bigrams we have? How many actual word bigrams we have? How do you explain this difference?\n",
    "3. What is the storage size of the input file \"lyrics.csv\"? What is the output file (bow.csv) size? how do you explain this difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words is:  105761\n",
      "Potenial word bigrams (worst case) is:  11185389121\n",
      "Actual word bigrams is:  2605158\n",
      "The storage size of bow.csv is:  85957502  Bytes\n",
      "The storage size of lyrcis.csv is:  168274338  Bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe lyrcis.csv has the full amount of words in the corpus, that includes words that appears more than once.\\nHowever, the bow.csv contain each word, that appears more than once in the corpus, only once (with the number of occurrences).\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "print ('The number of unique words is: ',len(tf_idf.inverted_index))\n",
    "\n",
    "\"\"\"\n",
    "When we initilize the inverted index we entered each word ones to the dictionary \n",
    "and just update the number of the occurance, Therfore all the words there are unique.\n",
    "In addition we could do that with the 'bow' object like that:\n",
    "data=pd.read_csv('bow.csv')\n",
    "df1=pd.DataFrame(data)\n",
    "print ('Number of unique words is: ',df1.shape[0])\n",
    "\"\"\"\n",
    "\n",
    "# 2.\n",
    "print ('Potenial word bigrams (worst case) is: ',((len(tf_idf.inverted_index))**2))\n",
    "print ('Actual word bigrams is: ',(len(tf_idf.bigram_count)))\n",
    "\n",
    "\"\"\"\n",
    "We'll compare the actual word biagrmas, which have only unique pairs combinations to the potential word biagram.\n",
    "Therfore, we want only the number of the unique pairs combinations. If we have V unique words in all of the corpus,\n",
    "we can say that the count of the potential word bigrams is V**2- This is the upper barrier.\n",
    "However, in the given data set, the number of pairs combinations is lower than this upper barrier, \n",
    "as there are repetitive word pairs combinations in the corpus.\n",
    "If we are talking not about the unique pairs cpombinations,so for one document we can get n-1 pairs combinations,\n",
    "when n is the number of all the words in the documents (not only the unique number of words).\n",
    "Therefore, for D documents we can get (n-1)*D pairs combinations (not unique).\n",
    "\"\"\"\n",
    "\n",
    "# 3.\n",
    "print ('The storage size of bow.csv is: ',BOW_PATH.stat().st_size, ' Bytes')\n",
    "print ('The storage size of lyrcis.csv is: ',INPUT_FILE_PATH.stat().st_size, ' Bytes')\n",
    "\n",
    "\"\"\"\n",
    "The lyrcis.csv has the full amount of words in the corpus, that includes words that appears more than once.\n",
    "However, the bow.csv contain each word, that appears more than once in the corpus, only once (with the number of occurrences).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NgramSpellingCorrector\n",
    "Now we will implement a Ngarm (character Ngrams) spelling corrector. That is, we have an out of vocabulary word (v) and we want to retrieve the most similar words (in our vocabulary) to this word.\n",
    "we will model the similarity of two words by-\n",
    "\n",
    "$$sim(v,w) := prior \\cdot likelihood = p(w) \\cdot P(v|w) $$ \n",
    "$$P(v|w) := JaccardIndex =  \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Where v is an out of vocabulary word (typo or spelling mistake), w is in a vocabulary word, X is the ngram set of v and Y is the ngram set of w.\n",
    "For example, if n == 3, the set of ngrams for word \"banana\" is set(\"ban\",\"ana\",\"nan\",\"ana\") = {\"ban\",\"ana\",\"nan\"}\n",
    "\n",
    "In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus to the words that those Ngrams appear in, in order prevent comparing v to all of the words in our corpus.\n",
    "Then, we will implement a function that computes this similarity.\n",
    "\n",
    "* Make sure you compute the JaccardIndex efficently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor example - get_bigrams is a generator, which is an object we can loop on:\\nfor ngram in get_bigrams(word):\\n    DO SOMETHING\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 2):\n",
    "        yield \"\".join(list(ngram))\n",
    "    \n",
    "\"\"\"\n",
    "for example - get_bigrams is a generator, which is an object we can loop on:\n",
    "for ngram in get_bigrams(word):\n",
    "    DO SOMETHING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSpellingCorrector:\n",
    "    def __init__(self, unigram_counts: Counter, get_n_gram: callable):\n",
    "        self.unigram_counts = unigram_counts\n",
    "        self.ngram_index = {}\n",
    "        self.get_n_grams = get_n_gram\n",
    "    \n",
    "    def build_index(self) -> None:\n",
    "        for word in self.unigram_counts.keys():                  # Goes all over the word in the unigram\n",
    "            word_ngrams_set=set(self.get_n_grams(word))               # Split the word by the get_n_gram which choosen, and turn it to set, which remove duplicate\n",
    "            for ngram in word_ngrams_set:                             # Go all over the ngram we got from the splitting\n",
    "                if ngram not in self.ngram_index:                # If ngram not yed appended to the dictionary ngram_index\n",
    "                    self.ngram_index[ngram]=[]                   # Initilize the value of this ngram as empty list\n",
    "                self.ngram_index[ngram].append(word)             # For each ngram key, append to the value the word which certainly contains this ngrm\n",
    "      \n",
    "    def get_top_k_words(self,word:str,k=5) -> (List[str], Dict[str, float]):\n",
    "        probabilties={}                                             # Initilize dictionary which contains the probabilty-sim for each word\n",
    "        word_ngrams_set=set(self.get_n_grams(word))                    # Split the big string by the n_gram which, and turn it to set, which remove duplicate\n",
    "        for ngram in word_ngrams_set:                                  # Goes over each ngram in the whole word\n",
    "            if ngram in self.ngram_index:                        # If ngram not in dictionary, it will continue\n",
    "                for word in self.ngram_index[ngram]:               # Goes over each word that contain this ngram\n",
    "                    if word not in probabilties:\n",
    "                        split_word=set(self.get_n_grams(word))     # Split this word by the n_gram which, and turn it to set, which remove duplicate\n",
    "                        intersection = len(list(set(word_ngrams_set).intersection(split_word)))          # Got the number of ngram that are in both the word and the big string\n",
    "                        union = (len(word_ngrams_set) + len(split_word)) - intersection              # Got the number of ngram that are in the word and in the big string\n",
    "                        similarity=(float(intersection)/union)\n",
    "                        probabilties[word]=similarity                        # Append the sim to the dictionary under the word (the key)\n",
    "        return (list(dict(Counter(probabilties).most_common(k)).keys()) , probabilties)                  # Return only the top K words, that for them the sim is biggest\n",
    "\n",
    "class BigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_vocab_word = 'acress'\n",
    "bigram_spelling_corrector = BigramSpellingCorrector(tf_idf.unigram_count)\n",
    "bigram_spelling_corrector.build_index()\n",
    "candidate_words, scores = bigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cress', 'recress', 'ress', 'actress', 'cresson']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccared score for the word cress is 0.8\n",
      "The jaccared score for the word recress is 0.67\n",
      "The jaccared score for the word ress is 0.6\n",
      "The jaccared score for the word actress is 0.57\n",
      "The jaccared score for the word cresson is 0.57\n"
     ]
    }
   ],
   "source": [
    "for word in candidate_words:\n",
    "    print(f'The jaccared score for the word {word} is {round(scores[word],2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Zipf's law \n",
    "For this part of the assignment, you will not ignore the “stop words” in advance, but rather take a data-driven approach. you will first create a dictionary with keys representing the words in the corpus and values representing how many times each word appears.\n",
    "\n",
    "Using a simple visualization (scatter plot), you will then check whether the frequency of the words corresponds to Zipf's law.  Once we see that this is indeed the case, we'll try to display the graph on a logarithmic scale (both x and y axis) and see if we can get a linear trend.\n",
    "\n",
    "Each word should then weighted according to its frequency, so that the words that appear most often are given a lower weight, and so on for the words whose frequency is the lowest.\n",
    "\n",
    "So how we are going to do that?\n",
    "You will standardize all the frequency values in the dictionary and after that, they will be re-weighted. If the value of the word is less than zero after standardization, you will calculate P(X <=Z) and if the value of the word is greater than 0, you will calculate 1 - P (X <= Z), and this will be the weight for every word.\n",
    "\n",
    "Now you will multiply the standard “tf” calculation we made at the beginning of the assignment by another coefficient, which is the weight we just calculated.\n",
    "\n",
    "Lastly, we will perform the same query you performed in the previous part of the assignment and check if the returned documents are identical or different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Override preprocess_sentence function in such a way that stop words are not ignored\n",
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        lower_word=word.lower()            # Lower case the word\n",
    "        word_letters = list(lower_word)\n",
    "        valid_letters = []\n",
    "        for l in word_letters:\n",
    "            if l in allowed_symbols:\n",
    "                valid_letters.append(l)  # Removes characters which are not in the allowed symbols\n",
    "        clean_word = ''.join(valid_letters)  # Reconnect the good letters back into one word\n",
    "        clean_word = stemmer.stem(clean_word) # stem of the word\n",
    "        if len(clean_word) > 1:             # Remove words that are 1 character long\n",
    "            output_sentence.append(clean_word)\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [33:05<00:00, 86.34s/it] \n"
     ]
    }
   ],
   "source": [
    "tf_idf_2 = TfIdf()\n",
    "tf_idf_2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalFrequency = tf_idf_2.unigram_count\n",
    "\n",
    "def find_k_most_frequant_words(k : int, totalFrequency: Dict[str,int]) -> Dict[str,int]:\n",
    "    top_k_frequant_words = dict(Counter(totalFrequency).most_common(k))\n",
    "    return top_k_frequant_words\n",
    "\n",
    "top_5_frequant_words = find_k_most_frequant_words(5, totalFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1343235, 'the': 1178419, 'to': 689489, 'and': 674981, 'it': 667416}\n"
     ]
    }
   ],
   "source": [
    "print(top_5_frequant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Word Frequency - scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_id = range(1, len(totalFrequency)+1)\n",
    "frequency_values = sorted(totalFrequency.values(), reverse=True)\n",
    "\n",
    "plt.scatter(word_id, frequency_values, s=2, alpha=0.6)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Your verbal solution here\\n\\n### End your verbal solution here\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Does the plot follow Zips' law?\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Word Frequency - scatterplot - log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 4))\n",
    "ax.scatter(word_id, frequency_values, s=2, alpha=0.6)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standartization and reweightening\n",
    "\n",
    "#### Standartization# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = statistics.mean(frequency_values)\n",
    "std =math.sqrt(statistics.variance(frequency_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standartization(totalFrequency, mean, std) -> Dict[str, int]:\n",
    "    standardized_dic = {}\n",
    "    for word in totalFrequency:\n",
    "        standardized_dic[word] = (totalFrequency[word] - mean) / std\n",
    "    return standardized_dic\n",
    "\n",
    "standardized_dic = standartization(totalFrequency, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a={'a':5,'d':6.3,'g':9}\n",
    "# mean = statistics.mean(a.values())\n",
    "# std =math.sqrt(statistics.variance(a.values()))\n",
    "\n",
    "# standardized_dic = standartization(a, mean, std)\n",
    "\n",
    "# print(statistics.mean(standardized_dic.values()))\n",
    "# print(math.sqrt(statistics.variance(standardized_dic.values())))\n",
    "\n",
    "# math.sqrt(statistics.variance(sorted(standardized_dic.values(), reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probability_of_zscore(standardized_dic) -> Dict[str,int]:\n",
    "    wighted_dic = {}\n",
    "    for key in standardized_dic.keys():\n",
    "        if  standardized_dic[key] <= 0:\n",
    "            wighted_dic[key] = st.norm(0, 1).cdf(standardized_dic[key])\n",
    "        else:\n",
    "            wighted_dic[key] = 1 - st.norm(0, 1).cdf(standardized_dic[key])\n",
    "    return wighted_dic\n",
    "\n",
    "weighted_dic = calc_probability_of_zscore(standardized_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Override the function update_inverted_index_with_tf_idf_and_compute_document_norm in such a way that you multiply the original tf by the weight of the word\n",
    "class WeightedTfIdf(TfIdf):\n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        for word in self.inverted_index.keys(): # For every word in the corpus\n",
    "            for doc in self.inverted_index[word].keys():  # For every document containing the word\n",
    "                n_words_in_doc = self.document_term_frequency[doc] # Length of the document is the number of words in it\n",
    "                tf = (self.inverted_index[word][doc]) / n_words_in_doc  # Term frequency is the number of words in a docuemt divided by the number of the words in it\n",
    "                idf = np.log10((self.n_docs + 1) / self.word_document_frequency[word])\n",
    "                w_ij = tf * idf * weighted_dic[word] # Calcluated tf-idf metric              \n",
    "                self.inverted_index[word][doc] = w_ij # Set the weight of a document to the tf-idf metric value   \n",
    "                if doc not in self.doc_norms:\n",
    "                    self.doc_norms[doc] = 0  # initialize value for the first time that creating the doc norm\n",
    "                self.doc_norms[doc] += (w_ij ** 2) # To calculate the norm, sum the square of the weights \n",
    "        for doc in self.doc_norms.keys():\n",
    "            self.doc_norms[doc] = np.sqrt(self.doc_norms[doc]) # The norm of the document is the square root of the weights' sum of squares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7/23 [2:46:11<6:19:51, 1424.45s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a971ffa66827>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mweightedTfIdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWeightedTfIdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mweightedTfIdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-976cdad385c7>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence_preprocesser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_counts_and_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_docs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-22784e3fbeb1>\u001b[0m in \u001b[0;36mpreprocess_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0moutput_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mlower_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m# Lower case the word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mword_letters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \"\"\"\n\u001b[1;32m-> 1272\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m-> 1326\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m-> 1326\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \"\"\"\n\u001b[0;32m   1356\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1332\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"next_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \"\"\"\n\u001b[0;32m   1377\u001b[0m         \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# used to ignore last token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1512\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4.1\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4.1\u001b[0m\u001b[1;36m.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \"\"\"\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    592\u001b[0m           \u001b[1;33m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \"\"\"\n\u001b[1;32m--> 594\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_tokenize_words\u001b[1;34m(self, plaintext)\u001b[0m\n\u001b[0;32m    563\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparastart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparastart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m                 \u001b[0mparastart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tok, **params)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weightedTfIdf = WeightedTfIdf(weighted_dic)\n",
    "weightedTfIdf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_dr = DocumentRetriever(weightedTfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_top_k = weighted_dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = weighted_dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there any similarity between the documents retrieved? Please explain.\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 bigram's language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following on from section 1.4, this section discusses spelling corrections.\n",
    "#### Documents were requested for the following query:\n",
    "\n",
    "\"the bad acress played role on the show\"\n",
    "\n",
    "#### It can be seen from section 1.4 that acress does not appear in the corpus and also received  five candidate words that could be considered its replacement. As soon as the sentence is given, it is possible and even desirable to consider the context in order to determine what candidate word to use. To do so, we will use bigram's language model to calculate P(w). Finally, we return the word with the best weighted score by multiplying P(w) by the jaccard index of the word. After you've done that, discuss the results.\n",
    "\n",
    "#### Notice: only p(w) is calculated in this case using the bigram language model a. You dont need to calculate p(x|w)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_uncorrect_sentense = \"the bad acress played role on the show\"\n",
    "\n",
    "#continue from here\n",
    "#use tf_idf object from section 1.1!!\n",
    "sentence = tf_idf.sentence_preprocesser(the_uncorrect_sentense)\n",
    "dict_scores = {}\n",
    "for i,word in enumerate(sentence):\n",
    "    if word not in tf_idf.unigram_count:\n",
    "        if i < len(sentence) - 1:\n",
    "            next_word = sentence[i+1]\n",
    "        else:\n",
    "            next_word = None\n",
    "        if i > 0:\n",
    "            previous_word = sentence[i-1]\n",
    "        else:\n",
    "            previous_word = None\n",
    "        candidate_words, scores = bigram_spelling_corrector.get_top_k_words(word)\n",
    "        for fixed_word in candidate_words:\n",
    "            if next_word:\n",
    "                next_prob = tf_idf.bigram_count[(fixed_word,next_word)] / tf_idf.unigram_count[fixed_word]\n",
    "            else:\n",
    "                next_prob = 1\n",
    "            if previous_word:\n",
    "                previous_prob = tf_idf.bigram_count[(previous_word,fixed_word)] / tf_idf.unigram_count[previous_word]\n",
    "            else:\n",
    "                previous_prob = 1\n",
    "            score = next_prob * previous_prob * scores[fixed_word]\n",
    "            dict_scores[fixed_word] = score\n",
    "        print(f'The new chosen word for the unidentified word *{word}* is {Counter(dict_scores).most_common(1)[0][0]}')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End - You did it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The time it took to run the entire code\n",
    "toc = time.perf_counter()\n",
    "print(f\"The time it took to run the entire code is: {(toc - tic)/60} minuts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
